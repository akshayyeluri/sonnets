{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(420)\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from HMM import unsupervised_HMM, from_hmm\n",
    "from HMM_helper import (\n",
    "    parse_seqs,\n",
    "    parse_text,\n",
    "    update_syll_map,\n",
    "    sample_sentence,\n",
    "    visualize_sparsities,\n",
    "    rhyme_dict_gen\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), 'data/shakespeare.txt'), 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'data/Syllable_dictionary.txt'), 'r') as f:\n",
    "    syll_map0 = {}\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0] \n",
    "        # Add 10 to denote end of line syll_count\n",
    "        sylls = np.array([int(s.replace('E', '1')) for s in line[1:]])\n",
    "        assert(word not in syll_map0)\n",
    "        syll_map0[word] = sylls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets = parse_text(text, by='sonnet')\n",
    "chars = sorted(list(set(\"\".join(sonnets))))\n",
    "char_to_int = dict((c, i) for i,c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i,c in enumerate(chars))\n",
    "num_chars = len(char_to_int)\n",
    "\n",
    "# source: https://blog.usejournal.com/how-to-develop-a-character-based-neural-language-model-99c18de1d4d2\n",
    "# organize into sequences of characters\n",
    "def make_data(step, length = 40):\n",
    "    char_seqs = list()\n",
    "    for curr in sonnets:\n",
    "        for i in range(length, len(curr), step):\n",
    "            # select sequence of tokens\n",
    "            seq = curr[i-length:i+1]\n",
    "            # store\n",
    "            char_seqs.append(seq)\n",
    "    print('Total Sequences: %d' % len(char_seqs))\n",
    "\n",
    "    # convert sequences of characters into sequences of integers using the mapping dictionary\n",
    "    int_seqs = list()\n",
    "    for seq in char_seqs:\n",
    "        new_seq = [char_to_int[char] for char in seq]\n",
    "        int_seqs.append(new_seq)\n",
    "    \n",
    "    int_seqs = np.array(int_seqs)\n",
    "    # last character is y, first 40 characters are x\n",
    "    train_X = int_seqs[:, :-1]\n",
    "    train_Y = int_seqs[:, -1]\n",
    "    return train_X, train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, seq_length, seed_text, n_chars, verbose = False):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for i in tqdm(range(n_chars)):\n",
    "        # encode the characters as integers\n",
    "        encoded = [char_to_int[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = np.array([encoded[-seq_length:]])\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(char_to_int))\n",
    "        # predict character\n",
    "        pred = model.predict_classes(encoded, verbose=0)[0]\n",
    "        # reverse map integer to character\n",
    "        out_char = int_to_char[pred]\n",
    "        in_text += out_char\n",
    "    if verbose:\n",
    "        print(\"Random seed: \" + in_text[0:seq_length])\n",
    "        print(\"Generated: \" + in_text[seq_length:])\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(step = 10, length = 40, epochs = 20):\n",
    "    train_X, train_Y = make_data(step, length = length)\n",
    "    ohe_X = np.array([to_categorical(x, num_classes = num_chars) for x in train_X])\n",
    "    ohe_Y = to_categorical(train_Y, num_classes = num_chars)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape = (ohe_X.shape[1], ohe_X.shape[2])))\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(ohe_X, ohe_Y, epochs = epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 27921\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 150)               108000    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 29)                4379      \n",
      "=================================================================\n",
      "Total params: 112,379\n",
      "Trainable params: 112,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27921 samples\n",
      "Epoch 1/40\n",
      "27921/27921 [==============================] - 74s 3ms/sample - loss: 2.5364 - accuracy: 0.2774\n",
      "Epoch 2/40\n",
      "27921/27921 [==============================] - 70s 3ms/sample - loss: 2.1543 - accuracy: 0.3664\n",
      "Epoch 3/40\n",
      "27921/27921 [==============================] - 73s 3ms/sample - loss: 2.0177 - accuracy: 0.3999s - loss: 2.0167 - accuracy: 0.\n",
      "Epoch 4/40\n",
      "27921/27921 [==============================] - 74s 3ms/sample - loss: 1.9188 - accuracy: 0.4220\n",
      "Epoch 5/40\n",
      "27921/27921 [==============================] - 79s 3ms/sample - loss: 1.8417 - accuracy: 0.4437\n",
      "Epoch 6/40\n",
      "27921/27921 [==============================] - ETA: 0s - loss: 1.7789 - accuracy: 0.45 - 86s 3ms/sample - loss: 1.7789 - accuracy: 0.4599\n",
      "Epoch 7/40\n",
      "27921/27921 [==============================] - 93s 3ms/sample - loss: 1.7254 - accuracy: 0.4728\n",
      "Epoch 8/40\n",
      "27921/27921 [==============================] - 107s 4ms/sample - loss: 1.6760 - accuracy: 0.4854\n",
      "Epoch 9/40\n",
      "27921/27921 [==============================] - 109s 4ms/sample - loss: 1.6275 - accuracy: 0.4984\n",
      "Epoch 10/40\n",
      "27921/27921 [==============================] - 113s 4ms/sample - loss: 1.5797 - accuracy: 0.5111\n",
      "Epoch 11/40\n",
      "27921/27921 [==============================] - 126s 5ms/sample - loss: 1.5314 - accuracy: 0.5246\n",
      "Epoch 12/40\n",
      "27921/27921 [==============================] - 133s 5ms/sample - loss: 1.4792 - accuracy: 0.5394\n",
      "Epoch 13/40\n",
      "27921/27921 [==============================] - 143s 5ms/sample - loss: 1.4268 - accuracy: 0.5515\n",
      "Epoch 14/40\n",
      "27921/27921 [==============================] - 153s 5ms/sample - loss: 1.3678 - accuracy: 0.57031:17 - los - ETA - ETA: 1:03 - loss: 1.3467 - accura - ETA: 58s - loss: 1.3480 - accuracy: 0.578 - ETA: 57s - loss: 1.3475 - accur - ETA: 56s - loss: 1.3488 - accuracy: 0.5 - ETA: 55s - loss: 1.3490 - accuracy: 0. - ETA: 54s - loss: 1 - ETA: 50s - loss: 1.34 - ETA: 4 - ETA: 41s - loss: 1.3596 - accuracy: - ETA: 39s - loss: 1.3610 - - ETA: 36s - loss: 1.3622 - accuracy: 0. - ETA: 36s - loss: 1.3618 - accuracy: 0.572 - ETA: 35s - loss: 1.3618 - accurac - ETA: 34s - loss: 1.3615 - - E - ETA: 1s - loss: 1.3679 - accura - ETA: 0s - loss: 1.3676 - accuracy\n",
      "Epoch 15/40\n",
      "27921/27921 [==============================] - 164s 6ms/sample - loss: 1.3091 - accuracy: 0.59092: - ETA:  - ETA: 2:19 - l - ETA: 2:09 - los - ETA: 2:03 - loss: 1.2652 - accuracy - ETA: 2:02 - loss: 1.2679 - ac - ETA: 2:01 - loss: 1.271 - - ETA: 1:55 - l - ETA: 1:53 - loss: 1.2772 - accuracy: 0. - ETA: 1:52 - loss: 1.2754 - accura - ETA: 1:52 - loss: 1.2791 - accura - ETA: 1:51 - loss: 1.2797 - ac - ETA: 1:49 - loss: 1.2808 - accuracy:  - ETA: 1:49 - loss: 1.2 - E - ETA: 1:44 - loss: 1. - ETA: 54s - l - ETA: 49s - loss: 1.3040 - ac - ETA: 46s - loss: 1.3033 - accuracy:   - ETA: 29s - l\n",
      "Epoch 16/40\n",
      "27921/27921 [==============================] - 175s 6ms/sample - loss: 1.2464 - accuracy: 0.6100\n",
      "Epoch 17/40\n",
      "27921/27921 [==============================] - 195s 7ms/sample - loss: 1.1785 - accuracy: 0.6333\n",
      "Epoch 18/40\n",
      "27921/27921 [==============================] - 211s 8ms/sample - loss: 1.1150 - accuracy: 0.6500\n",
      "Epoch 19/40\n",
      "27921/27921 [==============================] - 216s 8ms/sample - loss: 1.0488 - accuracy: 0.6753\n",
      "Epoch 20/40\n",
      "27921/27921 [==============================] - 222s 8ms/sample - loss: 0.9874 - accuracy: 0.6944\n",
      "Epoch 21/40\n",
      "27921/27921 [==============================] - 235s 8ms/sample - loss: 0.9255 - accuracy: 0.7131\n",
      "Epoch 22/40\n",
      "27921/27921 [==============================] - 247s 9ms/sample - loss: 0.8674 - accuracy: 0.7352\n",
      "Epoch 23/40\n",
      "27921/27921 [==============================] - 303s 11ms/sample - loss: 0.8086 - accuracy: 0.7540\n",
      "Epoch 24/40\n",
      "27921/27921 [==============================] - 269s 10ms/sample - loss: 0.7623 - accuracy: 0.7693\n",
      "Epoch 25/40\n",
      "27921/27921 [==============================] - 283s 10ms/sample - loss: 0.7130 - accuracy: 0.7852\n",
      "Epoch 26/40\n",
      "27921/27921 [==============================] - 282s 10ms/sample - loss: 0.6666 - accuracy: 0.8022\n",
      "Epoch 27/40\n",
      "27921/27921 [==============================] - 295s 11ms/sample - loss: 0.6281 - accuracy: 0.8149\n",
      "Epoch 28/40\n",
      "27921/27921 [==============================] - 305s 11ms/sample - loss: 0.5917 - accuracy: 0.8251\n",
      "Epoch 29/40\n",
      "27921/27921 [==============================] - 315s 11ms/sample - loss: 0.5579 - accuracy: 0.8366\n",
      "Epoch 30/40\n",
      "27921/27921 [==============================] - 328s 12ms/sample - loss: 0.5275 - accuracy: 0.8459\n",
      "Epoch 31/40\n",
      "27921/27921 [==============================] - 339s 12ms/sample - loss: 0.4965 - accuracy: 0.8585\n",
      "Epoch 32/40\n",
      "27921/27921 [==============================] - 341s 12ms/sample - loss: 0.4693 - accuracy: 0.8643\n",
      "Epoch 33/40\n",
      "27921/27921 [==============================] - 352s 13ms/sample - loss: 0.4459 - accuracy: 0.8707\n",
      "Epoch 34/40\n",
      "27921/27921 [==============================] - 363s 13ms/sample - loss: 0.4255 - accuracy: 0.8799\n",
      "Epoch 35/40\n",
      "27921/27921 [==============================] - 375s 13ms/sample - loss: 0.4107 - accuracy: 0.8837\n",
      "Epoch 36/40\n",
      "27921/27921 [==============================] - 383s 14ms/sample - loss: 0.3824 - accuracy: 0.8927\n",
      "Epoch 37/40\n",
      "27921/27921 [==============================] - 397s 14ms/sample - loss: 0.3746 - accuracy: 0.8947\n",
      "Epoch 38/40\n",
      "27921/27921 [==============================] - 406s 15ms/sample - loss: 0.3524 - accuracy: 0.9017\n",
      "Epoch 39/40\n",
      "27921/27921 [==============================] - 419s 15ms/sample - loss: 0.3411 - accuracy: 0.9052\n",
      "Epoch 40/40\n",
      "27921/27921 [==============================] - 428s 15ms/sample - loss: 0.3303 - accuracy: 0.9079\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn(step = 3, length = 40, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 27921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: o remove o no it is an ever-fixed mark t\n",
      "Generated: he sen my fair all thy hell of love mout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = make_data(3, 40)\n",
    "start = np.random.randint(0, len(train_X) - 1)\n",
    "pattern = ''.join([int_to_char[value] for value in train_X[start]])\n",
    "generate_seq(model, 40, pattern, 40, verbose = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
